{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b18a2e9-9b20-4a10-bbd7-f81deb479f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5519bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d74eb",
   "metadata": {},
   "source": [
    "### Generate synthetic data using GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d45e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample lists of data points\n",
    "organizations = [\n",
    "    \"Ocean Cleanup Initiative\", \"Marine Life Guardians\", \"Eco Ocean Warriors\", \"Sea Savers\",\n",
    "    \"Beach Protectors\", \"Wave Watchers\", \"Tide Turners\", \"Coastal Caretakers\"\n",
    "]\n",
    "locations = [\n",
    "    \"Coral Bay\", \"Blue Reef\", \"Marina Beach\", \"Sunset Shore\", \"Pelican Point\", \"Dolphin Cove\",\n",
    "    \"Seagull Island\", \"Mystic Beach\"\n",
    "]\n",
    "trash_types = [\n",
    "    \"Plastic Waste\", \"Metal Debris\", \"Assorted Trash\", \"Assorted Trash\", \"Industrial Waste\", \"Chemical Containers\", \n",
    "    \"Fishing Nets\", \"Glass Bottles\", \"Electronic Waste\", \"Rubber Tyres\", \"Textile Scraps\"\n",
    "]\n",
    "types = [\"instagram\", \"press_release\"]\n",
    "start_date = datetime(2017, 1, 1)\n",
    "\n",
    "# Function to generate a random date\n",
    "def generate_random_date():\n",
    "    random_days = random.randint(0, 365 * 5)  # Random date within 5 years\n",
    "    return (start_date + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Function to generate a random data point\n",
    "def generate_data_point():\n",
    "    return {\n",
    "        \"weight\": str(random.randint(100, 500)),  # Random weight between 100 and 500\n",
    "        \"organization\": random.choice(organizations),\n",
    "        \"date\": generate_random_date(),\n",
    "        \"location\": random.choice(locations),\n",
    "        \"trash_type\": random.choice(trash_types),\n",
    "        \"type\": random.choice(types)\n",
    "    }\n",
    "\n",
    "# Generate multiple data points\n",
    "num_data_points = 1000  # Adjust the number of data points as needed\n",
    "data_points = [generate_data_point() for _ in range(num_data_points)]\n",
    "\n",
    "# Save to a JSON file\n",
    "with open('data/data_points.json', 'w') as file:\n",
    "    json.dump(data_points, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e61134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Define the structure of your prompts\n",
    "prompt_structure = {\n",
    "    \"instagram\": \"Generate an Instagram caption for a beach cleanup where {weight} kilograms of {trash_type} were cleaned.\\nOrganization: {organization}\\nDate: {date}\\nLocation: {location}\",\n",
    "    \"press_release\": \"Generate a press release for a beach cleanup where {weight} kilograms of {trash_type} were cleaned.\\nOrganization: {organization}\\nDate: {date}\\nLocation: {location}\"\n",
    "}\n",
    "\n",
    "# Define the data points you want to use to generate the reports\n",
    "file_path = 'data/data_points.json'\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data_points = json.load(file)\n",
    "\n",
    "def remove_unicode_characters(text):\n",
    "    # Regex to match non-ASCII characters\n",
    "    non_ascii_pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "    # Remove non-ASCII characters\n",
    "    text = non_ascii_pattern.sub('', text)\n",
    "    # Replace newline characters with a space \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\\"', ' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "94d6884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reports saved to data/synthetic_reports.json\n"
     ]
    }
   ],
   "source": [
    "# Function to generate data\n",
    "def generate_data(data_points, prompt_structure, num_records):   \n",
    "    generated_data = []\n",
    "    for _ in range(num_records):\n",
    "            point = random.choice(data_points)\n",
    "            prompt_type = point['type']\n",
    "            prompt = prompt_structure[prompt_type].format(**point)\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=64\n",
    "            )\n",
    "            generated_text = remove_unicode_characters(response.choices[0].message.content.strip())\n",
    "            # Format the response into your desired structure\n",
    "            record = {\n",
    "                \"date\": point[\"date\"],\n",
    "                \"location\": point[\"location\"],\n",
    "                \"organization\": point[\"organization\"],\n",
    "                \"weight_kg\": point[\"weight\"],\n",
    "                \"trash_type\": point[\"trash_type\"],  # Extract from generated_text or define\n",
    "                \"caption\": generated_text\n",
    "            }\n",
    "            generated_data.append(record)\n",
    "\n",
    "    return generated_data\n",
    "\n",
    "# Generate and save the reports\n",
    "synthetic_reports = generate_data(data_points, prompt_structure, 100)\n",
    "\n",
    "filename = 'data/synthetic_reports.json'\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(synthetic_reports, file, indent=4)\n",
    "\n",
    "print(f\"Reports saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc356b",
   "metadata": {},
   "source": [
    "### Use a Pre-trained NER Model for Pre-annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22190502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8196c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pre_annotate(text):\n",
    "    doc = nlp(text)\n",
    "    labels = []\n",
    "    for token in doc:\n",
    "        if token.ent_iob_ != 'O':\n",
    "            labels.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "        else:\n",
    "            labels.append('O')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "75bcca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "with open('data/synthetic_reports.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Apply pre-annotation\n",
    "for item in data:\n",
    "    item['labels'] = pre_annotate(item['caption'])\n",
    "\n",
    "# Optionally, save the annotated data back to a file\n",
    "with open('data/synthetic_reports_annotated.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d386ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc760b",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2cf75a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeachCleanupDataset(Dataset):\n",
    "    def __init__(self, json_file, vocab, label_to_idx):\n",
    "        with open(json_file, 'r') as file:\n",
    "            self.data = json.load(file)\n",
    "        self.vocab = vocab\n",
    "        self.label_to_idx = label_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        caption_tokens = tokenize(item['caption'])\n",
    "        caption_indices = [self.vocab.get(token, 0) for token in caption_tokens]\n",
    "\n",
    "        if len(item['labels']) != len(caption_tokens):\n",
    "            print(\"Mismatch found in item:\", idx)\n",
    "            print(\"Caption:\", item['caption'])\n",
    "            print(\"Tokens:\", caption_tokens)\n",
    "            print(\"Labels:\", item['labels'])\n",
    "            raise AssertionError(\"Mismatch in tokens and labels length\")\n",
    "\n",
    "        labels = [self.label_to_idx[label] for label in item['labels']]\n",
    "\n",
    "        return torch.tensor(caption_indices), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e159749",
   "metadata": {},
   "source": [
    "### Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1aea4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple tokenizer\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Build a vocabulary from the dataset\n",
    "def build_vocab(data):\n",
    "    counter = Counter()\n",
    "    for item in data:\n",
    "        tokens = tokenize(item['caption'])\n",
    "        counter.update(tokens)\n",
    "    return {word: i+1 for i, (word, _) in enumerate(counter.most_common())}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "64ee09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab(data)\n",
    "\n",
    "# Define your labels based on the NER task\n",
    "unique_labels = set()\n",
    "for item in data:\n",
    "    unique_labels.update(item['labels'])\n",
    "\n",
    "# Now create label_to_idx with all unique labels\n",
    "label_to_idx = {label: idx for idx, label in enumerate(sorted(unique_labels))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2fdd9",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c6d3e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a60359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a63184",
   "metadata": {},
   "source": [
    "### Define the Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "401eadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for captions, labels in dataloader:\n",
    "            outputs = model(captions)\n",
    "\n",
    "            # Flatten outputs and labels\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions and true labels\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            valid_indices = labels != -1  # Indices where labels are not padding\n",
    "            valid_predictions = predictions[valid_indices]\n",
    "            valid_labels = labels[valid_indices]\n",
    "\n",
    "            all_predictions.extend(valid_predictions)\n",
    "            all_labels.extend(valid_labels)\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Ensure target_names match all labels in your dataset\n",
    "    unique_labels = sorted(set(all_labels + all_predictions))  # Combine and sort to get all unique labels\n",
    "    target_names = [label for label, idx in sorted(label_to_idx.items(), key=lambda item: item[1]) if idx in unique_labels]\n",
    "\n",
    "    # Update the classification report call\n",
    "    report = classification_report(all_labels, all_predictions, labels=unique_labels, target_names=target_names, zero_division=0)\n",
    "\n",
    "    \n",
    "    return average_loss, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06b6b1",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cc7eb6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_dataloader, eval_dataloader, criterion, optimizer, num_epochs, log_interval=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_idx, (captions, labels) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(captions)\n",
    "\n",
    "            # Flatten outputs and labels\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(f\"Epoch: {epoch+1}, Batch: {batch_idx}, Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "        average_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        eval_loss, eval_report = evaluate(model, eval_dataloader, criterion)\n",
    "\n",
    "        # Log epoch-level training and evaluation information\n",
    "        print(f\"End of Epoch {epoch + 1}\")\n",
    "        print(f\"Training Loss: {average_train_loss:.4f}\")\n",
    "        print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "#         print(\"Evaluation Report:\")\n",
    "#         print(eval_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14c20b",
   "metadata": {},
   "source": [
    "#### Define a custom collate function for the DataLoader to handle variable-length sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3a15c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    captions, labels = zip(*batch)\n",
    "    captions_padded = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
    "    return captions_padded, labels_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8cf71c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "95d9a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset\n",
    "with open('data/synthetic_reports_annotated.json', 'r') as file:\n",
    "    full_dataset = json.load(file)\n",
    "\n",
    "# Split the dataset into training and validation sets (80-20 split)\n",
    "train_data, val_data = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the split datasets if needed\n",
    "with open('data/train_data.json', 'w') as file:\n",
    "    json.dump(train_data, file)\n",
    "\n",
    "with open('data/val_data.json', 'w') as file:\n",
    "    json.dump(val_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0c8037b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training and validation datasets\n",
    "train_dataset = BeachCleanupDataset('data/train_data.json', vocab, label_to_idx)\n",
    "val_dataset = BeachCleanupDataset('data/val_data.json', vocab, label_to_idx)\n",
    "\n",
    "# Initialize the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c7e2901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Training Loss: 3.1357\n",
      "End of Epoch 1\n",
      "Training Loss: 3.1357\n",
      "Evaluation Loss: 3.0901\n",
      "Epoch: 2, Batch: 0, Training Loss: 3.0901\n",
      "End of Epoch 2\n",
      "Training Loss: 3.0901\n",
      "Evaluation Loss: 3.0439\n",
      "Epoch: 3, Batch: 0, Training Loss: 3.0439\n",
      "End of Epoch 3\n",
      "Training Loss: 3.0439\n",
      "Evaluation Loss: 2.9967\n",
      "Epoch: 4, Batch: 0, Training Loss: 2.9967\n",
      "End of Epoch 4\n",
      "Training Loss: 2.9967\n",
      "Evaluation Loss: 2.9480\n",
      "Epoch: 5, Batch: 0, Training Loss: 2.9480\n",
      "End of Epoch 5\n",
      "Training Loss: 2.9480\n",
      "Evaluation Loss: 2.8974\n",
      "Epoch: 6, Batch: 0, Training Loss: 2.8974\n",
      "End of Epoch 6\n",
      "Training Loss: 2.8974\n",
      "Evaluation Loss: 2.8445\n",
      "Epoch: 7, Batch: 0, Training Loss: 2.8445\n",
      "End of Epoch 7\n",
      "Training Loss: 2.8445\n",
      "Evaluation Loss: 2.7887\n",
      "Epoch: 8, Batch: 0, Training Loss: 2.7887\n",
      "End of Epoch 8\n",
      "Training Loss: 2.7887\n",
      "Evaluation Loss: 2.7296\n",
      "Epoch: 9, Batch: 0, Training Loss: 2.7296\n",
      "End of Epoch 9\n",
      "Training Loss: 2.7296\n",
      "Evaluation Loss: 2.6667\n",
      "Epoch: 10, Batch: 0, Training Loss: 2.6667\n",
      "End of Epoch 10\n",
      "Training Loss: 2.6667\n",
      "Evaluation Loss: 2.5994\n",
      "Epoch: 11, Batch: 0, Training Loss: 2.5994\n",
      "End of Epoch 11\n",
      "Training Loss: 2.5994\n",
      "Evaluation Loss: 2.5272\n",
      "Epoch: 12, Batch: 0, Training Loss: 2.5272\n",
      "End of Epoch 12\n",
      "Training Loss: 2.5272\n",
      "Evaluation Loss: 2.4497\n",
      "Epoch: 13, Batch: 0, Training Loss: 2.4497\n",
      "End of Epoch 13\n",
      "Training Loss: 2.4497\n",
      "Evaluation Loss: 2.3663\n",
      "Epoch: 14, Batch: 0, Training Loss: 2.3663\n",
      "End of Epoch 14\n",
      "Training Loss: 2.3663\n",
      "Evaluation Loss: 2.2768\n",
      "Epoch: 15, Batch: 0, Training Loss: 2.2768\n",
      "End of Epoch 15\n",
      "Training Loss: 2.2768\n",
      "Evaluation Loss: 2.1811\n",
      "Epoch: 16, Batch: 0, Training Loss: 2.1811\n",
      "End of Epoch 16\n",
      "Training Loss: 2.1811\n",
      "Evaluation Loss: 2.0793\n",
      "Epoch: 17, Batch: 0, Training Loss: 2.0793\n",
      "End of Epoch 17\n",
      "Training Loss: 2.0793\n",
      "Evaluation Loss: 1.9722\n",
      "Epoch: 18, Batch: 0, Training Loss: 1.9722\n",
      "End of Epoch 18\n",
      "Training Loss: 1.9722\n",
      "Evaluation Loss: 1.8613\n",
      "Epoch: 19, Batch: 0, Training Loss: 1.8613\n",
      "End of Epoch 19\n",
      "Training Loss: 1.8613\n",
      "Evaluation Loss: 1.7491\n",
      "Epoch: 20, Batch: 0, Training Loss: 1.7491\n",
      "End of Epoch 20\n",
      "Training Loss: 1.7491\n",
      "Evaluation Loss: 1.6396\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions for your model\n",
    "embedding_dim = 100  # Example dimension\n",
    "hidden_dim = 64      # Example dimension\n",
    "num_classes = len(label_to_idx)  # Number of unique NER labels\n",
    "\n",
    "model = NERModel(len(vocab)+1, embedding_dim, hidden_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "train_and_evaluate(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "33f4899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 1.6395673751831055\n",
      "Evaluation Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   B-CARDINAL       0.00      0.00      0.00         4\n",
      "       B-DATE       0.00      0.00      0.00         1\n",
      "        B-FAC       0.00      0.00      0.00         5\n",
      "        B-GPE       0.00      0.00      0.00        10\n",
      "        B-LOC       0.00      0.00      0.00         1\n",
      "      B-MONEY       0.00      0.00      0.00        12\n",
      "       B-NORP       0.00      0.00      0.00        11\n",
      "        B-ORG       0.00      0.00      0.00         3\n",
      "     B-PERSON       0.00      0.00      0.00        15\n",
      "    B-PRODUCT       0.00      0.00      0.00         2\n",
      "   B-QUANTITY       0.00      0.00      0.00         6\n",
      "B-WORK_OF_ART       0.00      0.00      0.00         9\n",
      "       I-DATE       0.00      0.00      0.00         1\n",
      "        I-FAC       0.00      0.00      0.00        16\n",
      "        I-GPE       0.00      0.00      0.00        12\n",
      "        I-LOC       0.00      0.00      0.00         4\n",
      "      I-MONEY       1.00      0.60      0.75        15\n",
      "        I-ORG       0.69      1.00      0.82       265\n",
      "\n",
      "     accuracy                           0.70       392\n",
      "    macro avg       0.09      0.09      0.09       392\n",
      " weighted avg       0.51      0.70      0.58       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evauate the model\n",
    "eval_loss, eval_report = evaluate(model, val_dataloader, criterion)\n",
    "print(f\"Evaluation Loss: {eval_loss}\")\n",
    "print(\"Evaluation Report:\")\n",
    "print(eval_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae35a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
